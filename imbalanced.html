<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Yannan Pan, 05/02/2018" />


<title>Cost-sensitive learning for Imbalanced Classification</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Yannan Pan</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Consulting</li>
    <li>
      <a href="consulting.html">Carp Movement</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">NLP</li>
    <li>
      <a href="tfidf.html">Tf-Idf in R</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Others</li>
    <li>
      <a href="imbalanced.html">Imbalanced Classification</a>
    </li>
  </ul>
</li>
<li>
  <a href="resume.html">Resume</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/yannanpan">
    <span class="fa fa-github-square"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/yannanpan/">
    <span class="fa fa-linkedin"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Cost-sensitive learning for Imbalanced Classification</h1>
<h4 class="author"><em>Yannan Pan, 05/02/2018</em></h4>

</div>


<hr />
<div id="imbalanced-classification" class="section level1">
<h1>Imbalanced Classification</h1>
<p>Imbalance is common in classification problems. Usually and without loss of generality, the minority class is treated as the positive class and it is more costly to make a false-negative mistake.</p>
<p>For example, we can define a cost matrix for binary classification problems, where <span class="math inline">\(c(i,j)\)</span> denotes the cost of classifying an observation from its true class <span class="math inline">\(j\)</span> into its predicted class <span class="math inline">\(i\)</span>. For an imbalanced classification, <span class="math inline">\(c(0,1)\)</span> is usually much higher than <span class="math inline">\(c(1,0)\)</span>.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>True positive</th>
<th>True negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Predict positive</strong></td>
<td>c(1,1)</td>
<td>c(1,0)</td>
</tr>
<tr class="even">
<td><strong>Predict negative</strong></td>
<td>c(0,1)</td>
<td>c(0,0)</td>
</tr>
</tbody>
</table>
<p>Ideally, the cost can be determined by domain experts. However, cost values may not be available for some real-world problems. In this case, we can assign cost values to be roughly the number of instances of the opposite class.</p>
<p><br></p>
</div>
<div id="cost-sensitive-learning" class="section level1">
<h1>Cost-sensitive Learning</h1>
<p>So, how can we handle an imbalanced dataset?</p>
<p>I have been struggling with this problem for a while. Some people suggested tree-methods such as boosting or bagging, while other people voted for sampling. With so many techniques I can utilize, I feel a little “spoilt for choice” and do not know where to start.</p>
<p>Fortunately, <a href="http://www.csd.uwo.ca/faculty/ling/papers/cost_sensitive.pdf">Ling and Sheng (2008)</a> provided a structure of cost-sensitive learning system for class imbalance problems. Here, I will summarize the structure in their paper and briefly explain each method.</p>
<p><br></p>
<div id="direct-methods" class="section level2">
<h2>1. Direct Methods</h2>
<p>Direct methods will build classifiers that directly minimize the misclassification costs. In other words, direct methods will take into consideration the cost while building models.</p>
<ul>
<li><p><a href="https://cling.csd.uwo.ca/papers/ICML04-Ling.pdf">Ling et al. (2004)</a> proposed a method which uses a criterion of minimal total cost on training data, instead of minimal entropy, to build decision trees. However, I did not find any library of this algorithm. If you do, please let me know!</p></li>
<li><p><a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf">Chen et al. (2004)</a> proposed a way to incorporate class weights into Random Forest, thus making it cost-sensitive. Specifically, class weights are used to weight the Gini criterion for finding splits during the tree growing process; class weights are also taken into consideration in the terminal nodes of each tree, i.e. the class prediction of each terminal node is determined by “weighted majority vote”. Free softwares can be downloaded <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">here</a>.</p></li>
<li><p>CART, penalized-SVM and C5.0 are some common approaches that can incorportate costs into the training process.</p></li>
</ul>
<p><br></p>
</div>
<div id="meta-learning" class="section level2">
<h2>2. Meta-learning</h2>
<p>Different from direct methods, meta-learning methods do NOT handle costs in the learning process. Meta-learning methods build cost-sensitive classifiers by pre-processing the training data or post-processing the output, i.e. sampling or thresholding. Technically speaking, meta-learning can convert any cost-insensitive algorithm into a cost-sensitive one.</p>
<p><br></p>
<div id="a.-sampling" class="section level3">
<h3>a. Sampling</h3>
<p>Sampling can be used to balance the dataset before we implement classification algorithms. It can alter the class distribution based on the cost matrix. Some common sampling methods include oversampling, undersampling and SMOTE. Take undersampling as an example. If we keep all positive instances, then the number of negative instances should be multiplied by <span class="math inline">\(\frac{FP}{FN}\)</span>, which is less than 1 as usually <span class="math inline">\(FP &lt; FN\)</span>. In general, positive and negative instances are sampled by the ratio of <span class="math display">\[\frac{1}{FP/FN} = \frac{FN}{FP}\]</span></p>
<p>Weighting can also be viewed as a sampling method, which assigns unequal weights to instances based on the misclassification cost.</p>
<p><br></p>
</div>
<div id="b.-thresholding" class="section level3">
<h3>b. Thresholding</h3>
<p>Thresholding can be used to relabel the class after a classifier is built. It requires the output of classifiers to be probabilities.</p>
<ol style="list-style-type: lower-roman">
<li>Theoretical Thresholding</li>
</ol>
<p>If a classifier can produce accurate probability estimations, we can use a theoretical threshold to classify instances. The theoretical threshold is defined based on the cost matrix.</p>
<p><span class="math display">\[p = \frac{c(1,0)}{c(0,1)+c(1,0)}=\frac{FP}{FP+FN}\]</span></p>
<ol start="2" style="list-style-type: lower-roman">
<li>Empirical Thresholding</li>
</ol>
<p>Empirical thresholding does not require an accurate estimation of probabilities - an accurate ranking will be sufficient. It will search the best probability from the training set and uses that probability as the threshold to classify test instances. How we define this best probability depends on our criterion. It can be the threshold that minimizes the misclassification cost. It can also be the threshold that produces the largest area under the ROC curve.</p>
<ul>
<li><a href="https://www.aaai.org/Papers/AAAI/2006/AAAI06-076.pdf">Sheng and Ling (2006)</a> proposed a thresholding method that can be applied to any cost-insensitive classifier.</li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="example" class="section level1">
<h1>Example</h1>
<p>Let us use an example to address the class imbalance. The analyzed dataset was a 1994 German credit dataset from the research of Dr. Hans Hofmann from the UCI Machine Learning repository. This dataset consisted of 1000 observations and 20 attributes without any missing values. The response is a binary variable which describes each customer as good or bad.</p>
<pre class="r"><code>library(caret)
library(pROC)
library(C50)
library(DMwR)
## Read data
german = read.table(&#39;code/germancredit.txt&#39;)
colnames(german)[21] = &#39;score&#39;
german$score = factor(german$score, levels=c(1,2), labels=c(&quot;good&quot;,&quot;bad&quot;))
table(german$score) # original class distribution</code></pre>
<pre><code>## 
## good  bad 
##  700  300</code></pre>
<p>700 of the customers were considered good and 300 were considered bad, which indicates this is an imbalanced dataset. We can define a cost matrix using the number of examples in each class.</p>
<pre class="r"><code>## Cost matrix
cost_mat &lt;- matrix(c(0, 3, 7, 0), nrow = 2)
rownames(cost_mat) &lt;- colnames(cost_mat) &lt;- levels(german$score)
cost_mat</code></pre>
<pre><code>##      good bad
## good    0   7
## bad     3   0</code></pre>
<p>For the thresholding approach, the threshold should be determined using a different set (evaluation set) other than the training set and the test set. Here, I split the whole dataset into three parts, training:evaluation:test = 7:1:2.</p>
<pre class="r"><code>## Training-evaluation-test split
set.seed(2426)
train_idx = createDataPartition(german$score, p=0.7)[[1]]
training = german[train_idx,]
other = german[-train_idx,]

eval_idx = createDataPartition(other$score, p=1/3)[[1]]
evaluation = other[eval_idx,]
test = other[-eval_idx,]</code></pre>
<div id="baseline" class="section level2">
<h2>1.Baseline</h2>
<p>First, let us define training parameters and train a random forest model to serve as the baseline.</p>
<pre class="r"><code>## CV
cvCtrl &lt;- trainControl(method = &quot;cv&quot;,
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE)

## Baseline
set.seed(789)
rfFit &lt;- train(score ~., data = training,
                 method = &#39;rf&#39;,
                 trControl = cvCtrl,
                 ntree = 1000,
                 tuneLength = 10,
                 metric = &quot;ROC&quot;)

baseProb &lt;- predict(rfFit, newdata = test, type = &quot;prob&quot;)[,1]
basePred &lt;- predict(rfFit, newdata = test)
baseCM &lt;- confusionMatrix(basePred, test$score)
baseCM$table</code></pre>
<pre><code>##           Reference
## Prediction good bad
##       good  127  36
##       bad    13  24</code></pre>
</div>
<div id="sampling" class="section level2">
<h2>2. Sampling</h2>
<p>For the sampling method, I will use a hybrid sampling method – SMOTE. Different from oversampling, which oversamples the minority class with replacement, SMOTE generates synthetic examples from the minority class.</p>
<pre class="r"><code>## Sampling
set.seed(111)
smoteTrain &lt;- SMOTE(score ~ ., data=training)
table(smoteTrain$score) # class distribution after sampling</code></pre>
<pre><code>## 
## good  bad 
##  840  630</code></pre>
<pre class="r"><code>set.seed(789)
smoteFit &lt;- train(score ~., data = smoteTrain,
                  method = &#39;rf&#39;,
                  trControl = cvCtrl,
                  ntree = 1000,
                  tuneLength = 10,
                  metric = &quot;ROC&quot;)

smotePred &lt;- predict(smoteFit, newdata = test)
smoteCM &lt;- confusionMatrix(smotePred, test$score)
smoteCM$table</code></pre>
<pre><code>##           Reference
## Prediction good bad
##       good  103  24
##       bad    37  36</code></pre>
</div>
<div id="thresholding" class="section level2">
<h2>3. Thresholding</h2>
<p>For the thresholding method, I will use the baseline model to predict the evaluation set and find the best threshold that is closest to the perfect model (specificity = sensitivity =1). Then I will relabel the predicted class from the baseline model using the new threshold.</p>
<pre class="r"><code>## Thresholding
evalResults &lt;- data.frame(score = evaluation$score)
evalResults$RF &lt;- predict(rfFit, newdata = evaluation, type = &quot;prob&quot;)[,1]

rfROC &lt;- roc(evalResults$score, evalResults$RF,
               levels = rev(levels(evalResults$score)))
# best threshold
rfThresh &lt;- coords(rfROC, x = &quot;best&quot;, best.method = &quot;closest.topleft&quot;)  
# predict the test set using the new threshold
threPred &lt;- factor(ifelse(baseProb &gt; rfThresh[1],
                          &quot;good&quot;, &quot;bad&quot;),
                   levels = levels(evalResults$score))
threCM &lt;- confusionMatrix(threPred, test$score)
threCM$table</code></pre>
<pre><code>##           Reference
## Prediction good bad
##       good   95  15
##       bad    45  45</code></pre>
</div>
<div id="direct-methods-1" class="section level2">
<h2>4. Direct methods</h2>
<p>For direct methods, I will use a C5.0 model. Since the predict function for C5.0 does not produce probabilities when employing costs, we cannot use ROC as the metric, so I will use Kappa instead. Besides, C5.0 function can only have 100 boosting iterations at most.</p>
<pre class="r"><code>## C5.0
cvCtrlNoProb &lt;- cvCtrl
cvCtrlNoProb$summaryFunction &lt;- defaultSummary
cvCtrlNoProb$classProbs &lt;- FALSE

set.seed(789)
c5Fit &lt;- train(score ~., data = training,
               method = &#39;C5.0&#39;,
               trControl = cvCtrlNoProb,
               tuneGrid = data.frame(.model = &quot;tree&quot;,
                                     .trials = c(1:100),
                                     .winnow = FALSE),
               metric = &quot;Kappa&quot;,
               cost = cost_mat)

c5Pred &lt;- predict(c5Fit, newdata = test)
c5CM &lt;- confusionMatrix(c5Pred, test$score)
c5CM$table</code></pre>
<pre><code>##           Reference
## Prediction good bad
##       good  131  35
##       bad     9  25</code></pre>
</div>
<div id="comparison" class="section level2">
<h2>5. Comparison</h2>
<p>Here, we only tested various approaches on 1/10 of the whole dataset and there are many random procedures involved. To compare these methods and get a consistent result, I would repeat all the above steps times and take the average of the test set cost.</p>
<pre class="r"><code>source(&quot;code/costsensitive.R&quot;)
res = test_cost_repeat(10)
apply(res, 2, mean)</code></pre>
<pre><code>##     Baseline        SMOTE thresholding         C5.0 
##        300.9        276.6        264.5        303.9</code></pre>
</div>
<div id="next-steps" class="section level2">
<h2>6. Next steps</h2>
<p>In terms of the method comparison, the cross-validation might be a better way compared to repeated sampling. My next step would be to use an external cross-validation to validate all the instances.</p>
<p><br></p>
<hr />
</div>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<ol style="list-style-type: decimal">
<li><p>Ling, C. X., &amp; Sheng, V. S. Cost-sensitive learning and the class imbalance problem. 2008.</p></li>
<li><p>Chen, C., Liaw, A., &amp; Breiman, L. (2004). Using random forest to learn imbalanced data. <em>University of California, Berkeley, 110</em>, 1-12.</p></li>
<li><p>Sheng, V. S., &amp; Ling, C. X. (2006, July). Thresholding for making classifiers cost-sensitive. In <em>AAAI</em> (pp. 476-481).</p></li>
<li><p>Kuhn, M., &amp; Johnson, K. (2013). Applied predictive modeling (Vol. 26). New York: Springer.</p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
